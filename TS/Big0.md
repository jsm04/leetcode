# Big O notation

Big O notation is a mathematical notation that describes the limiting behavior
of a function when the argument tends towards a particular value or infinity.

In computer science, big O notation is used to classify algorithms according to
how their run time or space requirements grow as the input size grows.

O(1) O(log 1) O(n) O(n log n) O(n^2) O(2^n) (O n!)
